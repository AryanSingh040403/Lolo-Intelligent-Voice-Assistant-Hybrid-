# ğŸ§  Lolo v2.0 â€” Hybrid Low-Latency LLM Agent

A **production-grade hybrid LLM agent** designed for **real-time voice interaction**, combining **local cognition (RAG, tools)** with a **highâ€‘performance LLM inference server**. The system is architected with **latency as a firstâ€‘class constraint**, achieving nearâ€‘human conversational fluency through concurrency, optimized model serving, and intelligent agent routing.

---

## ğŸš€ Project Overview

**Lolo v2.0** upgrades a traditional desktop voice assistant into a **cognitive hybrid AI agent** capable of:

* Realâ€‘time speech understanding and response
* Intelligent decisionâ€‘making via agentic routing
* Grounded answers using local Retrievalâ€‘Augmented Generation (RAG)
* Desktop and utility control through function calling

ğŸ”‘ **Primary Objective:**

> **Lowâ€‘latency conversational fluency with Timeâ€‘toâ€‘Firstâ€‘Audio (TTFA) â‰¤ 500 ms**

This objective directly drives the systemâ€™s **concurrent, multiâ€‘threaded pipeline** and **local-first architecture**.

---

## ğŸ§  Cognitive Architecture

The agent dynamically routes user intent through one of three execution paths:

1. **RAG Tooling** â€” For domainâ€‘specific or documentâ€‘grounded queries
2. **Function Calling** â€” For deterministic desktop or utility actions
3. **LLM Reasoning** â€” For general conversational intelligence

This **agentic rerouting** ensures correctness, speed, and grounded responses while minimizing unnecessary LLM computation.

---

## âš™ï¸ Technology Stack

### ğŸ§  Core AI Models

![Qwen](https://img.shields.io/badge/LLM-Qwen1.5--1.8B--Chat-4B0082?style=for-the-badge\&logo=openai\&logoColor=white)
![MiniLM](https://img.shields.io/badge/Embeddings-all--MiniLM--L6--v2-0A66C2?style=for-the-badge)
![Whisper](https://img.shields.io/badge/ASR-faster--whisper-FF6F00?style=for-the-badge)
![XTTS](https://img.shields.io/badge/TTS-Coqui--XTTS--v2.2-8A2BE2?style=for-the-badge)

### âš™ï¸ Frameworks & Systems

![vLLM](https://img.shields.io/badge/vLLM-High--Throughput--Serving-006400?style=for-the-badge)
![LangChain](https://img.shields.io/badge/LangChain-Agentic--Routing-2F855A?style=for-the-badge)
![FAISS](https://img.shields.io/badge/FAISS-Vector--Search-0467DF?style=for-the-badge)
![Docker](https://img.shields.io/badge/Docker-GPU--Deployment-2496ED?style=for-the-badge\&logo=docker\&logoColor=white)

### ğŸ§ª MLOps & Deployment

![QLoRA](https://img.shields.io/badge/QLoRA-4--bit--Quantization-B83280?style=for-the-badge)
![PEFT](https://img.shields.io/badge/PEFT-Adapter--Training-6A5ACD?style=for-the-badge)
![W\&B](https://img.shields.io/badge/Weights%20%26%20Biases-Monitoring-FFBE00?style=for-the-badge)
![NVIDIA](https://img.shields.io/badge/NVIDIA-GPU--Required-76B900?style=for-the-badge\&logo=nvidia\&logoColor=white)

---

## ğŸ”„ Inference & Execution Pipeline

The system follows a **strict execution order** to ensure stability and performance:

1. **Environment Reset** â€” Clean virtual environment
2. **Dependency Installation** â€” Versionâ€‘pinned stable stack
3. **QLoRA Fineâ€‘Tuning** â€” Functionâ€‘calling adapter training
4. **RAG Indexing** â€” Local FAISS index construction
5. **LLM Deployment** â€” vLLM GPU server via Docker
6. **Diagnostics** â€” Agent tracing & latency monitoring
7. **Live Execution** â€” Realâ€‘time voice agent

The live agent uses a **producerâ€“consumer concurrency model**, running **ASR, LLM inference, and TTS in parallel** to mask latency.

---

## ğŸ“Š Performance & Evaluation

Latency and correctness are treated as **core KPIs**:

| Metric                    | Dimension          | Target   |
| ------------------------- | ------------------ | -------- |
| **TTFA**                  | Endâ€‘toâ€‘End Latency | â‰¤ 500 ms |
| **TTFT**                  | LLM Responsiveness | â‰¤ 350 ms |
| **Tool Call Accuracy**    | Cognitive Routing  | â‰¥ 95%    |
| **Response Groundedness** | RAG Quality        | â‰¥ 0.90   |

Latency tracing and agent decision paths are monitored via **Weights & Biases (W&B)**.

---

## ğŸ§© Data Preparation & RAG Design

* **Recursive, tokenâ€‘aware chunking**
* Chunk size: **600 tokens**
* Overlap: **100 tokens**
* Optimized for **high recall** and **low retrieval latency**

FAISS indices are persisted locally to ensure **subâ€‘second similarity search** without network dependency.

---

## ğŸ› ï¸ Prerequisites

* **Python:** â‰¤ 3.12 *(Python â‰¥ 3.13 is incompatible)*
* **Hardware:** NVIDIA GPU (required)
* **OS:** Linux / Windows (with manual audio driver setup)

> âš ï¸ All dependencies **must** be installed using the pinned versions provided. Deviations may break compatibility.

---

## âš ï¸ Limitations & Disclaimer

* **Dependency Fragility:** The stack relies on strict version pinning (bitsandbytes, vLLM, Coqui TTS)
* **Cold Start Cost:** Initial model downloads exceed 4GB; subsequent runs are cached
* **Audio I/O:** `pyaudio` and `sounddevice` may require manual systemâ€‘level configuration

---

## ğŸ“Œ Why This Project Matters

This project demonstrates:

* Systemsâ€‘level thinking for **realâ€‘time AI**
* Practical **LLM optimization and deployment**
* Agentic reasoning beyond simple prompt pipelines
* Productionâ€‘style monitoring and evaluation

It is designed as a **foundation for research, openâ€‘source extension, and realâ€‘world GenAI systems**.

---

## ğŸ¤ Contributions

Contributions, discussions, and improvements are welcome. Feel free to open an issue or submit a pull request.
