Lolo v2.0: Hybrid Cognitive LLM Agent ğŸ¤–ğŸ™ï¸Lolo v2.0 is an advanced, low-latency voice assistant that transitions from simple desktop automation to a sophisticated Hybrid LLM Agent architecture. It utilizes a local Qwen1.5-1.8B model specialized via QLoRA and a private RAG pipeline to provide high-fidelity desktop control and domain-specific knowledge retrieval in both online (accelerated) and truly offline environments.1. Model Description: Qwen1.5-1.8B-ChatThe project is centered around the Qwen1.5-1.8B-Chat model, a transformer-based decoder-only language model.Parameters: ~1.2B non-embedding parameters.Context Window: Supports a stable 32K token context length.1Capabilities: Strong performance in multilingual tasks, mathematical reasoning (GSM8K: 38.4), and coding (HumanEval: 20.1).3Optimization: The model is specialized for this project via 4-bit QLoRA fine-tuning to ensure precise JSON output for function calling while maintaining a minimum VRAM footprint of ~2.9GB.22. Technical SpecificationsCore FrameworksAgentic Orchestration: LangChain (v0.3.7) using the Tool Calling paradigm for rapid cognitive routing.Inference Engine: vLLM (v0.11.0) serving an OpenAI-compatible API, leveraging PagedAttention to minimize Time-to-First-Token (TTFT).6Vector Database:(https://github.com/facebookresearch/faiss) for local, disk-persisted high-speed similarity search.8Voice I/O StackSTT (Speech-to-Text): faster-whisper 10 with Voice Activity Detection (VAD) filtering to reduce dead-air latency.11TTS (Text-to-Speech): Coqui XTTS-v2 (v0.20.0) implementing streaming inference (inference_stream) for near-instant auditory feedback.123. Performance and Evaluation (MLOps)The system is designed to meet strict conversational KPIs, monitored and validated via W&B Prompts and the WandbTracer.14MetricTargetMeasurement StrategyTTFA (Time to First Audio)$\le 500\text{ ms}$Measured from user End-of-Speech to initial TTS chunk playback.16TTFT (Time to First Token)$\le 350\text{ ms}$Pre-fill latency monitored via vLLM server logs.17Tool Call Accuracy$\ge 95\%$Evaluated via W&B traces for RAG vs. Desktop Tool selection precision.Groundedness Score$\ge 0.90$Validates that answers are derived strictly from the FAISS RAG index.184. Usage and PrerequisitesPrerequisitesHardware: NVIDIA GPU with at least 8GB VRAM (Required for vLLM and bitsandbytes).OS: Windows (PowerShell) or Linux.Python: $\le 3.12$ (Python 3.10 or 3.11 is recommended for library stability).Installation (LOLO V4 Stabilized)To resolve the complex "Dependency Hell" involving numpy and coqui-tts, follow this exact sequence:PowerShell# 1. Clean Environment
Remove-Item -Recurse -Force.venv
python -m venv.venv
.venv\Scripts\Activate.ps1

# 2. Install Stabilized Pins
pip install -r scripts/setup_requirements.txt
5. Inference Pipeline & WorkflowThe system follows a Three-Threaded Producer-Consumer pattern to mask computational latency 19:STT Producer Thread: Captures audio using PyAudio, transcribes in real-time via faster-whisper, and pushes text to the Agent Queue.LLM Agent Worker: Consumes text, uses cognitive routing to decide between RAG (Proprietary Docs) or Functions (Desktop Tools), and streams response tokens.TTS Consumer Thread: Performs Sentence Boundary Detection (SBD), receives text fragments, and streams synthesized audio chunks for playback.206. Project StructureLOLO_V4_PROJECT/â”œâ”€â”€ data/                          # Source Dataâ”‚   â”œâ”€â”€ domain_docs/               # Proprietary text for RAG (lolo_v2_architecture.txt)â”‚   â””â”€â”€ qwen_tool_data.jsonl       # Specialized Function Calling datasetâ”œâ”€â”€ models/                        # Generated Artifactsâ”‚   â”œâ”€â”€ faiss_index_local/         # Persistent Vector Storeâ”‚   â””â”€â”€ qwen_qlora_adapter/        # Fine-tuned PEFT weightsâ”œâ”€â”€ deployment/                    # Containerizationâ”‚   â”œâ”€â”€ Dockerfile                 # Multi-stage build with model cachingâ”‚   â””â”€â”€ vllm_start_server.sh       # Launch script for high-throughput LLM servingâ”œâ”€â”€ scripts/                       # Business Logicâ”‚   â”œâ”€â”€ setup_requirements.txt     # Stabilized dependency pinsâ”‚   â”œâ”€â”€ finetune_qwen_lora.py      # QLoRA script with metadata bypass patchâ”‚   â”œâ”€â”€ rag_pipeline.py            # Token-aware recursive indexing logicâ”‚   â”œâ”€â”€ agent_core.py              # Cognitive Agent routing logicâ”‚   â””â”€â”€ realtime_agent.py          # Concurrent Voice loop implementationâ””â”€â”€ README.md7. Limitations and DisclaimerDependency Fragility: This project relies on specific library versions (transformers==4.37.1, coqui-tts==0.20.0). Upgrading to Python 3.13+ may break bitsandbytes and soxr compatibility.Privacy: All processing (STT, RAG, LLM) is performed locally. However, ensure legal compliance if using voice recordings for data collection or W&B logging.Model Caching: The initial build/run will be slow as it downloads ~4GB of model weights. Subsequent launches are optimized for speed.
